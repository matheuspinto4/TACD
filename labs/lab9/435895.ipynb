{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "425ac6e9-8919-4c06-be1c-782241f835f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6f6b7d-1fdf-43a4-bd7c-03d90114d3fb",
   "metadata": {},
   "source": [
    "# Lab 9 - Multi-layer Perceptron Forward Pass & Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c1b83-fb9a-4321-882b-fd0c74d2ab1b",
   "metadata": {},
   "source": [
    "## Part I\n",
    "For this exercise you will implement a simple 2-layer perceptron with the forward pass and the backpropagation to learn the weights\n",
    "\n",
    "For the first part you'll build and train a 2-layer neural network that predicts the prices of houses, using the usual Boston housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7804bef6-2bd6-4d05-bc4e-f9d8325f12ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matheus Pinto\\AppData\\Local\\Temp\\ipykernel_17112\\1649349983.py:2: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  boston = pd.read_csv(path,  delim_whitespace=True)\n"
     ]
    }
   ],
   "source": [
    "path = 'data/BostonHousing.txt'\n",
    "boston = pd.read_csv(path,  delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bdfc73-7702-4a3c-af9f-6fb897ec643c",
   "metadata": {
    "tags": []
   },
   "source": [
    "As usual, consider the MEDV as your target variable. \n",
    "* Split the data into training, validation and testing (70,15,15)%\n",
    "* Experiment with different number of neurons per layer for your network, using the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35513b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.values[:,:-1]\n",
    "y = boston.iloc[:,-1:]\n",
    "X = np.array(X, dtype=float)\n",
    "y = np.array(y, dtype=float)\n",
    "X_tv, X_test, y_tv, y_test = train_test_split(X, y,test_size=0.15, random_state=189, shuffle=True)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_tv, y_tv, test_size=0.15/0.85 , random_state=123, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df5bfc8-26c0-4d48-9c3c-05a9e404e1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid_activation(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_inverted(z):\n",
    "    sig = sigmoid_activation(z)\n",
    "    return sig*(1 - sig)\n",
    "\n",
    "def identity(z):\n",
    "    return z\n",
    "\n",
    "def standardize(X, mean=None, std=None):\n",
    "    X = np.array(X, dtype=float)\n",
    "    if mean is None or std is None:\n",
    "        mean = np.mean(X, axis=0)  # Per feature\n",
    "        std = np.std(X, axis=0)\n",
    "        std = np.where(std == 0, 1, std)  # Avoid division by zero\n",
    "    X_scaled = (X - mean) / std\n",
    "    return X_scaled, mean, std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9be3fcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_perceptron(X, W_list, activation, outputActivation):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of a two-layer fully connected perceptron.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : a 2-dimensional array\n",
    "        the input data\n",
    "    W_list : list\n",
    "        An list with teh weights matrices\n",
    "    activation : function\n",
    "        the activation function to be used for the hidden layer\n",
    "    outputActivation : funcition\n",
    "        the activation function to be used for the output layer\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : float\n",
    "        the output of the computation of the forward pass of the network\n",
    "    \"\"\"\n",
    "    # pushing the column of bias on X:\n",
    "    X = np.array(X, dtype=float)\n",
    "    X_bias = np.c_[X, np.ones(X.shape[0])]\n",
    "    W_1 = W_list[0]\n",
    "    W_2 = W_list[1]\n",
    "\n",
    "    A1 = X_bias @ W_1\n",
    "    Z1 = activation(A1)\n",
    "    Z1 = np.c_[Z1, np.ones(Z1.shape[0])]\n",
    "    Y = Z1 @ W_2\n",
    "\n",
    "    y_pred = outputActivation(Y)\n",
    "    return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ae3f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE_foward_pass(y_pred, y_target):\n",
    "    if (type(y_target) == pd.core.frame.DataFrame):\n",
    "        y_target = np.array(y_target)\n",
    "\n",
    "    dif = y_pred - y_target\n",
    "    dif_squared = dif * dif\n",
    "    RMSE = (dif_squared.sum() / dif_squared.size) ** (1 / 2)\n",
    "\n",
    "    return RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc26211-91c1-47f9-8779-eb723b0c209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_two_layer_perceptron(X, T, activation, outputActivation, dim_input, dim_hidden, dim_output, maxIter=100, learning_rate = 1e-3):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of a two-layer fully connected perceptron.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : a 2-dimensional array\n",
    "        the input data\n",
    "    activation : function\n",
    "        the activation function to be used for the hidden layer\n",
    "    dim_input : int\n",
    "        the dimensionality of the input layer\n",
    "    dim_hidden : int\n",
    "        the dimensionality of the hidden layer\n",
    "    dim_output : int\n",
    "        the dimensionality of the output layer\n",
    "    maxIter : int\n",
    "        the max number of iterations\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : float\n",
    "        the output of the computation of the forward pass of the network\n",
    "    \"\"\"\n",
    "    # pushing the column of bias on X:\n",
    "    X = np.array(X, dtype=float)\n",
    "    T = np.array(T, dtype=float)\n",
    "    X_bias = np.c_[X, np.ones(X.shape[0])]\n",
    "    \n",
    "    # Initializing the weights with random:\n",
    "    # We have two layers, so\n",
    "    np.random.seed(42)\n",
    "    W_1 = np.random.randn(dim_input + 1, dim_hidden) * np.sqrt(2.0 / (dim_input + dim_hidden))\n",
    "    W_2 = np.random.randn(dim_hidden + 1, dim_output) * np.sqrt(2.0 / (dim_hidden + dim_output))\n",
    "    \n",
    "    print(X_bias.shape)\n",
    "    print(W_1.shape)\n",
    "    for iteration in range(maxIter):\n",
    "        # Computing the foward pass:\n",
    "        A1 = X_bias @ W_1   \n",
    "        Z1 = activation(A1)\n",
    "        Z1 = np.c_[Z1, np.ones(Z1.shape[0])]\n",
    "        A2 = Z1 @ W_2\n",
    "        Y = outputActivation(A2)\n",
    "\n",
    "        # Computing the errors 'delta': \n",
    "        Delta_2 = Y - T\n",
    "        # Delta_1 = sigmoid_inverted(A1)\n",
    "        Delta_1 = (Delta_2 @ W_2[:-1,:].T) * sigmoid_inverted(A1)\n",
    "       \n",
    "        # Adjusting the wheights:\n",
    "\n",
    "        W_2 -= learning_rate*(Z1.T @ Delta_2)\n",
    "        W_1 -= learning_rate*(X_bias.T @ Delta_1)\n",
    "\n",
    "    return Y, [W_1, W_2] \n",
    "        \n",
    "\n",
    "dim_input = X_train.shape[1]\n",
    "dim_hidden = 32\n",
    "dim_output = 1\n",
    "X_train_scaled, train_mean, train_std = standardize(X_train)\n",
    "y, W_list = train_two_layer_perceptron(X_train_scaled, y_train, sigmoid_activation, identity, dim_input, dim_hidden, dim_output, maxIter=1000, learning_rate=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46bcbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4350121192066574\n"
     ]
    }
   ],
   "source": [
    "# Testing:\n",
    "X_test_stand, _, _ = standardize(X_test, mean=train_mean, std=train_std)\n",
    "y_pred_test = two_layer_perceptron(X_test_stand, W_list, sigmoid_activation, identity)\n",
    "print(RMSE_foward_pass(y_test,y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b85b3bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurônios ocultos: 1 = 5.823651888537768\n",
      "Neurônios ocultos: 10 = 3.5719031655752533\n",
      "Neurônios ocultos: 20 = 3.3323005590158536\n",
      "Neurônios ocultos: 32 = 5.970269468562682\n",
      "Neurônios ocultos: 64 = 8.11303159997741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matheus Pinto\\AppData\\Local\\Temp\\ipykernel_17112\\1409163214.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurônios ocultos: 128 = 21.92576382372181\n"
     ]
    }
   ],
   "source": [
    "dim_input = X_train.shape[1]\n",
    "dim_output = 1\n",
    "\n",
    "for dim_hidden in [1, 10, 20, 32, 64, 128]:\n",
    "    X_train_scaled, train_mean, train_std = standardize(X_train)\n",
    "    y, W_list = train_two_layer_perceptron(X_train_scaled, y_train, sigmoid_activation, identity, dim_input, dim_hidden, dim_output, maxIter=1000, learning_rate=1e-3)\n",
    "    # Testing:\n",
    "    X_test_stand, _, _ = standardize(X_test, mean=train_mean, std=train_std)\n",
    "    y_pred_test = two_layer_perceptron(X_test_stand, W_list, sigmoid_activation, identity)\n",
    "    print(f\"Neurônios ocultos: {dim_hidden} = {RMSE_foward_pass(y_test,y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6650ab-79e5-4636-a4c9-84b977c48541",
   "metadata": {},
   "source": [
    "## Part II \n",
    "\n",
    "For this exercise you will build and train a 2-layer neural network that predicts the exact digit from a hand-written image, using the MNIST dataset. \n",
    "For this exercise, add weight decay to your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bfe2473-16e8-4dce-9e5b-7d5ce1154200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f8f04ee-c8e6-4531-9ad4-b0e530e1f92d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c49e3cd-e16a-4847-ac73-b9628f3f159a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "172b3419-d470-433f-87f9-4df67e4761e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.shape\n",
    "X = np.array(X, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdaf177-1cda-4c04-8d12-090678310602",
   "metadata": {},
   "source": [
    "Again, you will split the data into training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8519363c-f7e0-43a8-ba4e-a33ab9d5b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tv2, X_test2, y_tv2, y_test2 = train_test_split(X, y,test_size=0.15, random_state=189, shuffle=True)\n",
    "X_train2, X_validation2, y_train2, y_validation2 = train_test_split(X_tv2, y_tv2, test_size=0.15/0.85 , random_state=123, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d29e1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_activation(z):\n",
    "    z = np.exp(z)\n",
    "    softmax = z / np.sum(z, axis=1, keepdims=True)\n",
    "\n",
    "    return softmax\n",
    "\n",
    "def adjust_y_target(y_target):\n",
    "    if (type(y_target) == pd.core.frame.DataFrame):\n",
    "        y_target = np.array(y_target)\n",
    "    Y_target = np.zeros(shape=(y_target.shape[0], 10))\n",
    "    for l in range(y_target.shape[0]):\n",
    "        Y_target[l, y_target[l]] = 1.\n",
    "    \n",
    "    return Y_target\n",
    "\n",
    "\n",
    "def cross_entropy_loss(Y_pred, Y_target):\n",
    "    E = -np.sum(Y_target * np.log(Y_pred)) / Y_pred.shape[0]\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "015f701c-f816-4208-8cdc-4ba32f03f08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6237390029850745\n",
      "0.09846686318026181\n",
      "0.03735177181221458\n",
      "0.02093960672925308\n",
      "0.014893741768411578\n",
      "0.011604991619001519\n",
      "0.00950541769232497\n",
      "0.008022276691455573\n",
      "0.006919287409911968\n",
      "0.006071962230965158\n"
     ]
    }
   ],
   "source": [
    "def multiclass_two_layer_perceptron_weight_decay(X, T, dim_hidden, dim_output,activation, outputActivation, lmb1, lmb2, learning_rate=1e-3, maxIter=100):\n",
    "    X = np.array(X, dtype=float)\n",
    "    dim_input = X.shape[1]\n",
    "\n",
    "    T = adjust_y_target(T)\n",
    "\n",
    "    # Adding the bias\n",
    "    X_bias = np.c_[X, np.ones(X.shape[0])]\n",
    "    \n",
    "    # Initializing the weights with random numbers:\n",
    "    # We have two layers, so\n",
    "    np.random.seed(42) \n",
    "    W_1 = np.random.randn(dim_input + 1, dim_hidden) * np.sqrt(2.0 / (dim_input + dim_hidden))\n",
    "    W_2 = np.random.randn(dim_hidden + 1, dim_output) * np.sqrt(2.0 / (dim_hidden + dim_output))\n",
    "    \n",
    "    for iteration in range(maxIter):\n",
    "        # Computing the foward pass:\n",
    "        A1 = X_bias @ W_1   \n",
    "        Z1 = activation(A1)\n",
    "        Z1 = np.c_[Z1, np.ones(Z1.shape[0])]\n",
    "        A2 = Z1 @ W_2\n",
    "        Y = outputActivation(A2)\n",
    "\n",
    "        # Computing the errors 'delta': \n",
    "        Delta_2 = Y - T\n",
    "\n",
    "        Delta_1 = (Delta_2 @ W_2[:-1,:].T) * sigmoid_inverted(A1)\n",
    "\n",
    "        # Adjusting the wheights:\n",
    "\n",
    "        W_2[:-1, :] -= learning_rate*(Z1.T @ Delta_2 + lmb2 * W_2)[:-1,:]\n",
    "        W_2[-1, :] -= learning_rate*(Z1.T @ Delta_2)[-1, :]\n",
    "        W_1[:-1, :] -= learning_rate*(X_bias.T @ Delta_1 + lmb1 * W_1)[:-1, :]\n",
    "        W_1[-1, :] -= learning_rate*(X_bias.T @ Delta_1)[-1, :]\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "            print(cross_entropy_loss(Y, T))\n",
    "\n",
    "    return Y, [W_1, W_2] \n",
    "\n",
    "\n",
    "dim_hidden=64\n",
    "dim_output=len(np.unique(y))\n",
    "\n",
    "\n",
    "y_pred_train, W_list_msti = multiclass_two_layer_perceptron_weight_decay(X_train2, y_train2, dim_hidden, dim_output,sigmoid_activation, softmax_activation, lmb1=1e-3, lmb2=1e-4, learning_rate=1e-3, maxIter=1000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4476c49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12156794047663314\n"
     ]
    }
   ],
   "source": [
    "def forward_pass_multi_class(X, activation, activation_output, W_list):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of a two-layer fully connected perceptron.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : a 2-dimensional array\n",
    "        the input data\n",
    "    activation : function\n",
    "        the activation function to be used for the hidden layer\n",
    "    activation_output : function\n",
    "        the activation function to be used for the output layer\n",
    "    dim_input : int\n",
    "        the dimensionality of the input layer\n",
    "    dim_hidden : int\n",
    "        the dimensionality of the hidden layer\n",
    "    dim_output : int\n",
    "        the dimensionality of the output layer\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : nd.array\n",
    "        the output of the computation of the forward pass of the network\n",
    "    \"\"\"\n",
    "    W_1 = W_list[0]\n",
    "    W_2 = W_list[1]\n",
    "    \n",
    "    # pushing the column of bias on X:\n",
    "    X = np.array(X, dtype=float)\n",
    "    X_bias = np.c_[X, np.ones(X.shape[0])]\n",
    "\n",
    "\n",
    "    A1 = X_bias @ W_1\n",
    "    Z1 = activation(A1)\n",
    "    Z1 = np.c_[Z1, np.ones(Z1.shape[0])]\n",
    "    Y = Z1 @ W_2\n",
    "\n",
    "    y_pred = activation_output(Y)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "dim_input = X_train.shape[1]\n",
    "dim_hidden = 64\n",
    "dim_output = 10\n",
    "y_pred2 = forward_pass_multi_class(X_test2, sigmoid_activation,softmax_activation, W_list_msti)\n",
    "# print(y_pred2)\n",
    "y_test2 = adjust_y_target(y_test2)\n",
    "print(cross_entropy_loss(y_pred2,y_test2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VENV_algoritmos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
